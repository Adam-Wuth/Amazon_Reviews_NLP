{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00410b1-9cc2-4eed-86e0-b6d2cea9bf3a",
   "metadata": {},
   "source": [
    "# Amazon Review Star Rating Prediction – RNN Model\n",
    "Author: Aiden Devine  \n",
    "Model: BiLSTM with GloVe embeddings (50d)  \n",
    "Input: Tokenized review text  \n",
    "Output: Predicted star rating (1–5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68a2c9-d3bf-46d4-96ca-3b290fc42b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, copy, json\n",
    "import torch, torch.nn as nn, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78149c9a-baf0-412a-8c2d-4daac72f3cee",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d7f81-c990-4b7f-b566-4bee0cd6b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_from_disk(\"filetred_amazon_reviews\")\n",
    "print(len(reviews))\n",
    "print(Counter(reviews[\"rating\"]))\n",
    "print(reviews[0], '\\n')\n",
    "print(reviews[1])\n",
    "print(reviews.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa7db25-207c-4d3b-aa91-d315ef26d63a",
   "metadata": {},
   "source": [
    "### Load Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb23a5-a488-47ec-b8d0-fa8233edb8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = 'glove.6B.50d.txt' # modify to appropriate path for your file system\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "with open(glove_file, 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip().split(' ')\n",
    "        word = line[0]\n",
    "        embed = np.asarray(line[1:], \"float\")\n",
    "\n",
    "        embeddings_dict[word] = embed\n",
    "\n",
    "\n",
    "print('Loaded {} words from glove'.format(len(embeddings_dict)))\n",
    "\n",
    "low = -1.0 / 3\n",
    "high = 1.0 / 3\n",
    "embedding_matrix = np.random.uniform(low=low, high=high, size=(len(embeddings_dict)+1, 50))\n",
    "\n",
    "word2id = {}\n",
    "for i, word in enumerate(embeddings_dict.keys(), 1):\n",
    "\n",
    "    word2id[word] = i                                \n",
    "    embedding_matrix[i] = embeddings_dict[word]      \n",
    "\n",
    "word2id['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285fb2a-1724-4b04-ad9f-3f94f49c3416",
   "metadata": {},
   "source": [
    "### Set up train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07452ee5-c85e-47d9-8669-72b8ce6fca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified from the HW_3 \n",
    "class RNNAmazonReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset=None, word2id=None, finalized_data=None, data_limit=None, max_length=128):\n",
    "        \"\"\"\n",
    "        :param hf_dataset: A Hugging Face Dataset object (preloaded and filtered)\n",
    "        :param word2id: The GloVe word2id dictionary\n",
    "        :param finalized_data: Used to create validation set or to tokenize data\n",
    "        :param data_limit: Max number of examples to use\n",
    "        :param max_length: Max sequence length\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.word2id = word2id\n",
    "\n",
    "        examples, labels = [], []\n",
    "\n",
    "        # Unified logic: load from finalized_data or hf_dataset\n",
    "        data_source = finalized_data if finalized_data else hf_dataset\n",
    "        limit = len(data_source) if data_limit is None else data_limit\n",
    "\n",
    "        for i, example in enumerate(data_source):\n",
    "            if i >= limit:\n",
    "                break\n",
    "            examples.append(example[\"text\"])\n",
    "            labels.append(int(example[\"rating\"]) - 1)  # 1–5 stars → 0–4\n",
    "\n",
    "        tokenized = self.tokenize(examples)\n",
    "        self.data = [(ids, length, label) for (ids, length), label in zip(tokenized, labels)]\n",
    "        random.seed(42)\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "    def tokenize(self, examples):\n",
    "        example_ids = []\n",
    "        misses = 0\n",
    "        total = 0\n",
    "        for example in tqdm(examples):\n",
    "            tokens = word_tokenize(example)\n",
    "            ids = []\n",
    "            for tok in tokens:\n",
    "                if tok in self.word2id:\n",
    "                    tok.lower() # change text to lowercase\n",
    "                    ids.append(self.word2id[tok])\n",
    "                else:\n",
    "                    misses += 1\n",
    "                    ids.append(self.word2id.get('unk', 0))\n",
    "                total += 1\n",
    "            \n",
    "            if len(ids) == 0:\n",
    "                continue\n",
    "            \n",
    "            if len(ids) >= self.max_length:\n",
    "                ids = ids[:self.max_length]\n",
    "                length = self.max_length\n",
    "            else:\n",
    "                length = len(ids)\n",
    "                ids += [self.word2id['<pad>']] * (self.max_length - len(ids))\n",
    "\n",
    "            example_ids.append((torch.tensor(ids), length))\n",
    "\n",
    "        print(f'Missed {misses} out of {total} words -- {misses/total:.2%}')\n",
    "        return example_ids\n",
    "\n",
    "    def generate_validation_split(self, ratio=0.8):\n",
    "        split_idx = int(ratio * len(self.data))\n",
    "        val_split = self.data[split_idx:]\n",
    "        self.data = self.data[:split_idx]\n",
    "        return val_split\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]  # returns (input_ids, length, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ae593-a49e-4488-8d8f-4f1e2125803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split_full_usage(hf_dataset, seed=42):\n",
    "    grouped_by_rating = defaultdict(list)\n",
    "\n",
    "    # Group by rating\n",
    "    for ex in hf_dataset:\n",
    "        rating = int(ex['rating']) - 1  # Convert 1–5 → 0–4\n",
    "        grouped_by_rating[rating].append(ex)\n",
    "\n",
    "    # Create fixed stratified splits\n",
    "    train_data, val_data, test_data = [], [], []\n",
    "\n",
    "    for rating, examples in grouped_by_rating.items():\n",
    "        if len(examples) < 20026:\n",
    "            raise ValueError(f\"Expected 20026 examples for rating {rating}, got {len(examples)}\")\n",
    "\n",
    "        random.seed(seed)\n",
    "        random.shuffle(examples)\n",
    "\n",
    "        n_train = int(0.80 * 20026)   # 16020 per class\n",
    "        n_val   = int(0.19 * 20026)   # 3804 per class\n",
    "        n_test  = 20026 - n_train - n_val  # 202 per class\n",
    "\n",
    "        train_data.extend(examples[:n_train])\n",
    "        val_data.extend(examples[n_train:n_train + n_val])\n",
    "        test_data.extend(examples[n_train + n_val:])\n",
    "\n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(val_data)\n",
    "    random.shuffle(test_data)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# split raw data\n",
    "train_data, val_data, test_data = stratified_split_full_usage(reviews)\n",
    "\n",
    "#tokenize each dataset\n",
    "train_dataset = RNNAmazonReviewDataset(finalized_data=train_data, word2id=word2id)\n",
    "valid_dataset = RNNAmazonReviewDataset(finalized_data=val_data, word2id=word2id)\n",
    "test_dataset  = RNNAmazonReviewDataset(finalized_data=test_data, word2id=word2id)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}  Val: {len(valid_dataset)}  Test: {len(test_dataset)}\")\n",
    "\n",
    "print(valid_dataset[0])  # (input_ids, length, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442cbf7c-2609-439d-8cd1-a4c757c2e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "input_ids, length, label = valid_dataset[1]\n",
    "print(\"True length:\", length)\n",
    "print(\"Non-padded input:\", input_ids[:length])\n",
    "print(\"Label:\", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8addb396-997d-4bbd-b7fc-3c35f8224998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix, lstm_hidden_size=75, num_lstm_layers=2, bidirectional=True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n",
    "        self.lstm = nn.LSTM(input_size = embedding_matrix.shape[1],\n",
    "                            hidden_size = lstm_hidden_size,\n",
    "                            num_layers = num_lstm_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            batch_first = True,\n",
    "                            dropout = 0.3)\n",
    "        \n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.hidden_1 = nn.Linear(lstm_hidden_size * self.num_directions, lstm_hidden_size)\n",
    "        self.hidden_2 = nn.Linear(lstm_hidden_size, 5) # final layer to 5 classes\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self, input_batch, input_lengths):\n",
    "                \n",
    "        embedded_input = self.embedding(input_batch)\n",
    "        \n",
    "        packed_input = pack_padded_sequence(embedded_input, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        \n",
    "        hn_view = hn.view(self.lstm.num_layers, self.num_directions, input_batch.shape[0], self.lstm.hidden_size)\n",
    "        \n",
    "        hn_view_last_layer = hn_view[-1]                                                                                       \n",
    "        \n",
    "        if self.num_directions == 2:\n",
    "            # bidirectional → concat forward and backward\n",
    "            hn_cat = torch.cat([hn_view_last_layer[0], hn_view_last_layer[1]], dim=1)\n",
    "        else:\n",
    "            # unidirectional → just take last layer output\n",
    "            hn_cat = hn_view_last_layer[-1]\n",
    "                                         \n",
    "        hid = self.relu(self.hidden_1(hn_cat))\n",
    "        hid = self.dropout(hid)\n",
    "        \n",
    "        output = self.hidden_2(hid)\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37168743-3192-4ce8-ac55-31605ae1c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model, test_dataset, batch_size=256):\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_data, length, y in test_dataloader:\n",
    "            logits = model(input_data, length)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            y_true.extend(y.tolist())\n",
    "            y_pred.extend(preds.tolist())\n",
    "\n",
    "    print(\"=== Test Set Evaluation ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"1★\", \"2★\", \"3★\", \"4★\", \"5★\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eea82f-73f8-463d-8b86-3b4324e30520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_classification(model, train_dataset, valid_dataset, epochs=10, batch_size=256, learning_rate=.001, print_frequency=25):\n",
    "    criteria = nn.CrossEntropyLoss() # changed from BCE\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print('Total train batches: {}'.format(train_dataset.__len__() / batch_size))\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_model_sd = None\n",
    "    \n",
    "    all_batch_losses = []\n",
    "    epoch_avg_losses = []\n",
    "    epoch_accuracies = []\n",
    "    epoch_val_losses = []\n",
    "    \n",
    "    # Uncomment to create directories for saved models and metrics\n",
    "    # os.makedirs(\"saved_models\", exist_ok=True)\n",
    "    # os.makedirs(\"saved_metrics\", exist_ok=True)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print('### Epoch: ' + str(i+1) + ' ###')\n",
    "    \n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        for step, data in enumerate(tqdm(train_dataloader, desc=f\"Training Epoch {i+1}\")):\n",
    "\n",
    "            x, x_lengths, y = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_output = model(x, x_lengths)\n",
    "\n",
    "            loss = criteria(model_output, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            all_batch_losses.append(loss.item())\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            if step % print_frequency == (print_frequency - 1):\n",
    "                print(f\"epoch: {i} batch: {step} loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_epoch_loss = total_loss / max(1, batch_count)\n",
    "        epoch_avg_losses.append(avg_epoch_loss)\n",
    "        \n",
    "        print('Evaluating...')\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_batch_count = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_correct = 0\n",
    "            total_examples = 0\n",
    "            for input_data, length, y in valid_dataloader:\n",
    "                logits = model(input_data, length)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                loss = criteria(logits, y)\n",
    "                total_val_loss += loss.item()\n",
    "                val_batch_count += 1\n",
    "\n",
    "                total_correct += (preds == y).sum().item()\n",
    "                total_examples += len(y)\n",
    "\n",
    "            acc = total_correct / total_examples\n",
    "            val_loss_avg = total_val_loss / max(1, val_batch_count)\n",
    "            epoch_val_losses.append(val_loss_avg)\n",
    "            print(f\"Validation Accuracy: {acc:.4f} | Validation Loss: {val_loss_avg:.4f}\")\n",
    "            epoch_accuracies.append(acc)\n",
    "            if acc > best_accuracy:\n",
    "                best_model_sd = copy.deepcopy(model.state_dict())\n",
    "                best_accuracy = acc\n",
    "         \n",
    "        # Uncomment to save model\n",
    "        # epoch_filename = f\"saved_models/model_epoch_{i+1}.pth\"\n",
    "        # torch.save(model.state_dict(), epoch_filename)\n",
    "        # print(f\"Saved model checkpoint to {epoch_filename}\")\n",
    "        \n",
    "        metrics = {\n",
    "            \"all_batch_losses\": all_batch_losses,\n",
    "            \"epoch_avg_losses\": epoch_avg_losses,\n",
    "            \"epoch_val_losses\": epoch_val_losses,\n",
    "            \"epoch_accuracies\": epoch_accuracies\n",
    "        }\n",
    "        # Uncomment to save metrics\n",
    "        # with open(\"saved_metrics/metrics_epoch_{:02d}.json\".format(i+1), \"w\") as f:\n",
    "        #    json.dump(metrics, f)\n",
    "\n",
    "    return model.state_dict(), best_model_sd, all_batch_losses, epoch_avg_losses, epoch_val_losses, epoch_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc64b6-1cfb-44fd-87dc-a6bf9de3bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(embedding_matrix, lstm_hidden_size=50, num_lstm_layers=2, bidirectional=True)\n",
    "\n",
    "final_model_state, best_model_state, all_losses, epoch_losses, epoch_val_losses, accuracies = train_lstm_classification(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    valid_dataset,\n",
    "    epochs=5,             \n",
    "    batch_size=256,\n",
    "    learning_rate=1e-3,\n",
    "    print_frequency=25\n",
    ")\n",
    "\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "for i, (train_loss, val_loss, acc) in enumerate(zip(epoch_losses, epoch_val_losses, accuracies), 1):\n",
    "    print(f\"Epoch {i:>2}: \"\n",
    "          f\"Train Loss = {train_loss:.4f} | \"\n",
    "          f\"Val Loss = {val_loss:.4f} | \"\n",
    "          f\"Val Accuracy = {acc:.4f}\")\n",
    "\n",
    "evaluate_model_on_test(model, test_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1a7a4-c14f-44d3-a7a7-a254cb57db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save final model state\n",
    "\n",
    "#torch.save(final_model_state, 'final_amazon_review_model.pth')\n",
    "#print('Final model state saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a82b7-c4d1-4de9-8f21-ac4c632c4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you decided to save metrics, uncomment this to pull them from json \n",
    "    # and plot loss, average loss, and validation accuracy\n",
    "\n",
    "# with open(\"saved_metrics/metrics_epoch_05.json\") as f:\n",
    "#     metrics = json.load(f)\n",
    "\n",
    "# all_losses = metrics[\"all_batch_losses\"]\n",
    "# epoch_losses = metrics[\"epoch_avg_losses\"]\n",
    "# val_losses = metrics[\"epoch_val_losses\"]\n",
    "# accuracies = metrics[\"epoch_accuracies\"]\n",
    "\n",
    "# # Plot all batch-level training losses\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.plot(all_losses, label=\"Batch Loss\")\n",
    "# plt.title(\"Training Loss per Batch\")\n",
    "# plt.xlabel(\"Batch #\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot average loss per epoch\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(epoch_losses, marker='o', label=\"Avg Train Loss per Epoch\", color='orange')\n",
    "# plt.plot(val_losses, marker='x', label=\"Val Loss per Epoch\", color='red')\n",
    "# plt.title(\"Average Loss per Epoch\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot validation accuracy per epoch\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(accuracies, marker='o', label=\"Validation Accuracy\", color='green')\n",
    "# plt.title(\"Validation Accuracy per Epoch\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24305a-c084-4bbd-92c6-b145b63305fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you decided to save models as you trained, uncomment this to load weights from .pth and predict on test dataset\n",
    "    # Make sure the hidden size of 'model' matches the hidden size of your original saved model\n",
    "    # Also make sure the names match\n",
    "\n",
    "# # === Define your test dataloader ===\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "# # === Define your model parameters (must match training!) ===\n",
    "# def load_model(epoch):\n",
    "#     model = LSTMModel(embedding_matrix, lstm_hidden_size=75, num_lstm_layers=2, bidirectional=True)\n",
    "#     model_path = f\"saved_models/model_epoch_{epoch}.pth\"\n",
    "#     model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "# # === Evaluate all saved models ===\n",
    "# for epoch in range(1, 6):\n",
    "#     print(f\"\\n=== Evaluating model from epoch {epoch} ===\")\n",
    "#     model = load_model(epoch)\n",
    "\n",
    "#     y_true, y_pred = [], []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for input_data, length, y in test_dataloader:\n",
    "#             logits = model(input_data, length)\n",
    "#             preds = torch.argmax(logits, dim=1)\n",
    "#             y_true.extend(y.tolist())\n",
    "#             y_pred.extend(preds.tolist())\n",
    "\n",
    "#     print(classification_report(y_true, y_pred, target_names=[\"1★\", \"2★\", \"3★\", \"4★\", \"5★\"], zero_division=0))\n",
    "\n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
