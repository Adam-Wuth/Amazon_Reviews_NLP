{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project, Classification of Amazon Reviews and Key Phrases\n",
    "#### CSCI 3832 Natural Language Processing\n",
    "Members: Adam Wuth, Benjamin Kohav, Noah Vilas, Aiden Devine, Evan Zachary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, copy\n",
    "import torch, torch.nn as nn, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data set\n",
    "The dataset is split into categories, but we wanted all categories from 2020 onwards. This code block will take forever to run, only run it the first time to get the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #The dataset is split into categories\n",
    "\n",
    "# categories = [\n",
    "#     \"All_Beauty\",\n",
    "#     \"Amazon_Fashion\",\n",
    "#     \"Appliances\",\n",
    "#     \"Arts_Crafts_and_Sewing\",\n",
    "#     \"Automotive\",\n",
    "#     \"Baby_Products\",\n",
    "#     \"Beauty_and_Personal_Care\",\n",
    "#     \"Books\",\n",
    "#     \"CDs_and_Vinyl\",\n",
    "#     \"Cell_Phones_and_Accessories\",\n",
    "#     \"Clothing_Shoes_and_Jewelry\",\n",
    "#     \"Digital_Music\",\n",
    "#     \"Electronics\",\n",
    "#     \"Gift_Cards\",\n",
    "#     \"Grocery_and_Gourmet_Food\",\n",
    "#     \"Handmade_Products\",\n",
    "#     \"Health_and_Household\",\n",
    "#     \"Health_and_Personal_Care\",\n",
    "#     \"Home_and_Kitchen\",\n",
    "#     \"Industrial_and_Scientific\",\n",
    "#     \"Kindle_Store\",\n",
    "#     \"Magazine_Subscriptions\",\n",
    "#     \"Movies_and_TV\",\n",
    "#     \"Musical_Instruments\",\n",
    "#     \"Office_Products\",\n",
    "#     \"Patio_Lawn_and_Garden\",\n",
    "#     \"Pet_Supplies\",\n",
    "#     \"Software\",\n",
    "#     \"Sports_and_Outdoors\",\n",
    "#     \"Subscription_Boxes\",\n",
    "#     \"Tools_and_Home_Improvement\",\n",
    "#     \"Toys_and_Games\",\n",
    "#     \"Video_Games\",\n",
    "#     \"Unknown\"\n",
    "# ]\n",
    "\n",
    "# # #to get reviews from 2023 onwards 2020 onwards was millions of reviews and was taking\n",
    "# # #over an hour just to load the data\n",
    "# # start_timestamp = int(datetime(2023, 1, 1).timestamp() * 1000)\n",
    "\n",
    "# # #to store all datasets\n",
    "# # allcats = []\n",
    "\n",
    "# # for cat in categories:\n",
    "# #     print(f\"Loading category: {cat}\")\n",
    "# #     dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", f\"raw_review_{cat}\", split=\"full[:150000]\",  trust_remote_code=True)\n",
    "# #     #dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", f\"raw_review_{cat}\", split=\"full[:1%]\",  trust_remote_code=True)\n",
    "# #    #dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", cat, split=\"full\",  trust_remote_code=True) formatting issues\n",
    "# #     #get the 2023 onwards and add to data\n",
    "# #     filtered_dataset = dataset.filter(lambda x: x['timestamp'] >= start_timestamp)\n",
    "# #     #allcats.append(dataset)\n",
    "# #     allcats.append(filtered_dataset)\n",
    "# # #make one final dataset    \n",
    "# # reviews = concatenate_datasets(allcats)\n",
    "\n",
    "\n",
    "# limit = 589  # 100,000 target reviews 34 categories 5 stars, (100,000/34)/5 = 889\n",
    "\n",
    "# allcats = []\n",
    "\n",
    "# for cat in categories:\n",
    "#     print(f\"Loading category: {cat}\")\n",
    "#     #arbitrary 10000000 to make sure I get enough data after filter\n",
    "#     dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", f\"rawreview{cat}\", split=\"full[:1000000]\", trust_remote_code=True)\n",
    "#     #dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", f\"rawreview{cat}\", split=\"full[:1%]\",  trust_remote_code=True)\n",
    "#     #to get reviews from 2023 onwards 2020 onwards was millions of reviews and was taking\n",
    "#     #over an hour just to load the data\n",
    "#     filtered_dataset = dataset.filter(lambda x: x['timestamp'] >= int(datetime(2023, 1, 1).timestamp() * 1000))\n",
    "#     #in each category, for stars 1-5(not inclusive)\n",
    "#     for star in range(1, 6):\n",
    "#         data = dataset.filter(lambda x: int(float(x[\"rating\"])) == star)\n",
    "#         if len(data) >= limit:\n",
    "#             #trim extra reviews randomly to avoid bias\n",
    "#             data = data.shuffle().select(range(limit))\n",
    "#             allcats.append(data)\n",
    "# #Save reviews so we don't have to run code again\n",
    "# reviews = concatenate_datasets(allcats)\n",
    "# reviews.save_to_disk(\"filetred_amazon_reviews\")\n",
    "# print(Counter(reviews[\"rating\"])) \n",
    "# print(f\"Total reviews loaded: {len(reviews)}\")\n",
    "# Counter({1.0: 20026, 2.0: 20026, 3.0: 20026, 4.0: 20026, 5.0: 20026})\n",
    "\n",
    "\n",
    "\n",
    "# reviews = load_from_disk(\"filtered_amazon_reviews\")\n",
    "\n",
    "# print(f\"Total reviews loaded: {len(reviews)}\")\n",
    "\n",
    "# reviews.save_to_disk(\"filtered_amazon_reviews\")\n",
    "\n",
    "# print(Counter(reviews[\"rating\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have run that already, reviews was saved(should be in the working directory)so you can just do the next code block instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1.0: 20026, 2.0: 20026, 3.0: 20026, 4.0: 20026, 5.0: 20026})\n",
      "100130\n",
      "{'rating': 1.0, 'title': 'Worst nail polish ever', 'text': 'Worst nail polish ever! My daughter and I both used this nail polish in two different colors and now our nails are damaged. Our nails split horizontally and are peeling. Plus the damage has caused pain. Worst Sally Hansen product ever!', 'images': [], 'asin': 'B011855ADM', 'parent_asin': 'B011855ADM', 'user_id': 'AEMVAG56MA7MAFULCQJEOVJCKGHA', 'timestamp': 1454738837000, 'helpful_vote': 8, 'verified_purchase': True}\n",
      "{'rating': 1.0, 'title': 'No funciona para mi', 'text': 'Bueno en cuanto a mi respondo que no me funciono. Tengo pocas pestañas, las enchufe antes de poner la máscara y el resultado desastroso. El producto hizo que mis pestañas perdieran el volumen del encrespado horrible.', 'images': [], 'asin': 'B09GTV6WL6', 'parent_asin': 'B09GTV6WL6', 'user_id': 'AFPNHXMEBYKO3SPMFXZCALLZ5IHA', 'timestamp': 1645820993736, 'helpful_vote': 5, 'verified_purchase': True}\n",
      "['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "reviews = load_from_disk(\"./filtered_amazon_reviews\")\n",
    "\n",
    "print(Counter(reviews[\"rating\"]))\n",
    "\n",
    "\n",
    "print(len(reviews))\n",
    "print(reviews[0])\n",
    "print(reviews[1])\n",
    "print(reviews.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Loaded 400000 words from GloVe\n",
      "Unknown token index: 400000\n",
      "Padding token index: 400001\n",
      "\n",
      "Building bigram vocabulary and embeddings...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9050/2412397700.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBuilding bigram vocabulary and embeddings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mbigram2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_embedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_ngram_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bigram vocab size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- 1. Load GloVe Embeddings ----\n",
    "glove_file = \"glove.6B.50d.txt\"  # Update path if needed\n",
    "embeddings_dict = {}\n",
    "\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "\n",
    "with open(glove_file, 'r', encoding='utf8', errors=\"replace\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split(' ')\n",
    "        word = line[0]\n",
    "        embed = np.asarray(line[1:], dtype=np.float32)\n",
    "        if embed.shape[0] != 50:\n",
    "            continue\n",
    "        embeddings_dict[word] = embed\n",
    "\n",
    "print('Loaded {} words from GloVe'.format(len(embeddings_dict)))\n",
    "\n",
    "# ---- 2. Build Word Embedding Matrix ----\n",
    "embedding_dim = 50\n",
    "embedding_matrix = []\n",
    "word2id = {}\n",
    "\n",
    "for i, word in enumerate(embeddings_dict):\n",
    "    word2id[word] = i\n",
    "    embedding_matrix.append(embeddings_dict[word])\n",
    "\n",
    "# Add <unk> and <pad>\n",
    "word2id['<unk>'] = len(word2id)\n",
    "embedding_matrix.append(np.random.uniform(-0.1, 0.1, embedding_dim))\n",
    "\n",
    "word2id['<pad>'] = len(word2id)\n",
    "embedding_matrix.append(np.zeros(embedding_dim))\n",
    "\n",
    "embedding_matrix = np.stack(embedding_matrix)\n",
    "\n",
    "print(\"Unknown token index:\", word2id['<unk>'])\n",
    "print(\"Padding token index:\", word2id['<pad>'])\n",
    "\n",
    "# ---- 3. Build N-gram Embeddings (bi, tri, quad) ----\n",
    "\n",
    "def build_ngram_vocab(reviews, n, embeddings_dict):\n",
    "    ngram2id = {'<unk>': 0, '<pad>': 1}\n",
    "    ngram_embeddings = [\n",
    "        np.random.uniform(-0.1, 0.1, embedding_dim),  # <unk>\n",
    "        np.zeros(embedding_dim)                      # <pad>\n",
    "    ]\n",
    "\n",
    "#     for text in reviews[\"text\"]:\n",
    "    for text in tqdm(reviews[\"text\"], desc=f\"Building {n}-grams\"):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram_words = tokens[i:i+n]\n",
    "            ngram = \"_\".join(ngram_words)\n",
    "            if ngram not in ngram2id:\n",
    "                vectors = [embeddings_dict.get(w, embeddings_dict.get('<unk>', np.random.uniform(-0.1, 0.1, embedding_dim))) for w in ngram_words]\n",
    "                avg_embedding = np.mean(vectors, axis=0)\n",
    "                ngram2id[ngram] = len(ngram2id)\n",
    "                ngram_embeddings.append(avg_embedding)\n",
    "\n",
    "    ngram_embedding_matrix = np.stack(ngram_embeddings)\n",
    "    return ngram2id, ngram_embedding_matrix\n",
    "\n",
    "print(\"\\nBuilding bigram vocabulary and embeddings...\")\n",
    "bigram2id, bigram_embedding_matrix = build_ngram_vocab(reviews, n=2, embeddings_dict=embeddings_dict)\n",
    "print(\"Bigram vocab size:\", len(bigram2id))\n",
    "\n",
    "print(\"\\nBuilding trigram vocabulary and embeddings...\")\n",
    "trigram2id, trigram_embedding_matrix = build_ngram_vocab(reviews, n=3, embeddings_dict=embeddings_dict)\n",
    "\n",
    "print(\"Trigram vocab size:\", len(trigram2id))\n",
    "\n",
    "print(\"\\nBuilding quadgram vocabulary and embeddings...\")\n",
    "quadgram2id, quadgram_embedding_matrix = build_ngram_vocab(reviews, n=4, embeddings_dict=embeddings_dict)\n",
    "\n",
    "print(\"Quadgram vocab size:\", len(quadgram2id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class NgramReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, ngram2id, n=2, max_len=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            reviews: list of review texts\n",
    "            labels: list of corresponding labels\n",
    "            ngram2id: dictionary mapping n-gram strings to IDs\n",
    "            n: size of the n-grams (e.g., 2 for bigrams, 3 for trigrams, etc.)\n",
    "            max_len: maximum sequence length (padded or truncated)\n",
    "        \"\"\"\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.ngram2id = ngram2id\n",
    "        self.n = n\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        tokens = word_tokenize(text.lower())\n",
    "\n",
    "        # Generate n-grams\n",
    "        ngrams = [tokens[i:i + self.n] for i in range(len(tokens) - self.n + 1)]\n",
    "\n",
    "        # Convert n-grams to string format and map to IDs\n",
    "        ngram_ids = []\n",
    "        for ng in ngrams:\n",
    "            ngram_str = \"_\".join(ng)\n",
    "            ngram_id = self.ngram2id.get(ngram_str, self.ngram2id.get('<unk>', 0))\n",
    "            ngram_ids.append(ngram_id)\n",
    "\n",
    "        # Pad or truncate\n",
    "        if len(ngram_ids) < self.max_len:\n",
    "            pad_id = self.ngram2id.get('<pad>', 0)\n",
    "            ngram_ids.extend([pad_id] * (self.max_len - len(ngram_ids)))\n",
    "        else:\n",
    "            ngram_ids = ngram_ids[:self.max_len]\n",
    "\n",
    "        # Convert to tensors\n",
    "        ngram_ids = torch.tensor(ngram_ids, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return ngram_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n=2] Total unique n-grams found: 121576\n",
      "[n=2] N-grams kept (freq ≥ 2): 33178\n",
      "[n=3] Total unique n-grams found: 248099\n",
      "[n=3] N-grams kept (freq ≥ 2): 32146\n",
      "[n=4] Total unique n-grams found: 310088\n",
      "[n=4] N-grams kept (freq ≥ 2): 16694\n",
      "Bigram Train: 6548 | Validation: 728\n",
      "Trigram Train: 6548 | Validation: 728\n",
      "Quadgram Train: 6548 | Validation: 728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.util import ngrams\n",
    "def build_ngram_vocab(reviews, n=2, min_freq=2, limit=100000):\n",
    "    counter = Counter()\n",
    "    for review in reviews[:limit]:\n",
    "        text = review['text'].lower() if isinstance(review, dict) else review.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        ngram_list = ngrams(tokens, n)\n",
    "        ngram_strs = [\"_\".join(ng) for ng in ngram_list]\n",
    "        counter.update(ngram_strs)\n",
    "\n",
    "    # Filter by frequency\n",
    "    vocab = {\n",
    "        ng: idx + 1 for idx, (ng, count) in enumerate(counter.items()) if count >= min_freq\n",
    "    }\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<unk>'] = len(vocab)\n",
    "\n",
    "    print(f\"[n={n}] Total unique n-grams found: {len(counter)}\")\n",
    "    print(f\"[n={n}] N-grams kept (freq ≥ {min_freq}): {len(vocab) - 2}\")  # -2 to exclude special tokens\n",
    "    return vocab\n",
    "\n",
    "\n",
    "review_texts = [r['text'] if isinstance(r, dict) else r for r in reviews]\n",
    "review_labels = [int(r['rating']) - 1 if isinstance(r, dict) else r for r in reviews]\n",
    "\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    review_texts, review_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "bigram2id = build_ngram_vocab(train_texts, n=2, min_freq=2)\n",
    "trigram2id = build_ngram_vocab(train_texts, n=3, min_freq=2)\n",
    "quadgram2id = build_ngram_vocab(train_texts, n=4, min_freq=2)\n",
    "\n",
    "\n",
    "train_bigram_dataset = NgramReviewDataset(train_texts, train_labels, bigram2id, n=2)\n",
    "val_bigram_dataset = NgramReviewDataset(val_texts, val_labels, bigram2id, n=2)\n",
    "\n",
    "train_trigram_dataset = NgramReviewDataset(train_texts, train_labels, trigram2id, n=3)\n",
    "val_trigram_dataset = NgramReviewDataset(val_texts, val_labels, trigram2id, n=3)\n",
    "\n",
    "train_quadgram_dataset = NgramReviewDataset(train_texts, train_labels, quadgram2id, n=4)\n",
    "val_quadgram_dataset = NgramReviewDataset(val_texts, val_labels, quadgram2id, n=4)\n",
    "\n",
    "\n",
    "print(f\"Bigram Train: {len(train_bigram_dataset)} | Validation: {len(val_bigram_dataset)}\")\n",
    "print(f\"Trigram Train: {len(train_trigram_dataset)} | Validation: {len(val_trigram_dataset)}\")\n",
    "print(f\"Quadgram Train: {len(train_quadgram_dataset)} | Validation: {len(val_quadgram_dataset)}\")\n",
    "\n",
    "\n",
    "def vectorize_review_ngrams(text, ngram2id, n=2):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    ngram_list = ngrams(tokens, n)\n",
    "    return [ngram2id.get(\"_\".join(ng), ngram2id.get('<unk>', 0)) for ng in ngram_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NgramClassifier(nn.Module):\n",
    "    def __init__(self, ngram_matrix, ngram2id, num_classes):\n",
    "        super(NgramClassifier, self).__init__()\n",
    "        vocab_size, embed_dim = ngram_matrix.shape\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(ngram_matrix, dtype=torch.float32),\n",
    "            padding_idx=ngram2id['<pad>']\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x).mean(dim=1)\n",
    "        return self.fc(embed)\n",
    "    \n",
    "def create_ngram_classifiers(bigram_data, trigram_data, quadgram_data, num_classes):\n",
    "    bigram_matrix, bigram2id = bigram_data\n",
    "    trigram_matrix, trigram2id = trigram_data\n",
    "    quadgram_matrix, quadgram2id = quadgram_data\n",
    "\n",
    "    return {\n",
    "        'bigram': NgramClassifier(bigram_matrix, bigram2id, num_classes),\n",
    "        'trigram': NgramClassifier(trigram_matrix, trigram2id, num_classes),\n",
    "        'quadgram': NgramClassifier(quadgram_matrix, quadgram2id, num_classes),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Training bigram model ====\n",
      "Epoch 1/5, Loss: 1.6196, Val Acc: 36.68%\n",
      "Epoch 2/5, Loss: 1.5607, Val Acc: 63.60%\n",
      "Epoch 3/5, Loss: 1.5077, Val Acc: 64.42%\n",
      "Epoch 4/5, Loss: 1.4603, Val Acc: 64.42%\n",
      "Epoch 5/5, Loss: 1.4182, Val Acc: 64.42%\n",
      "Final Validation Accuracy for bigram: 64.42%\n",
      "\n",
      "==== Training trigram model ====\n",
      "Epoch 1/5, Loss: 1.6101, Val Acc: 23.63%\n",
      "Epoch 2/5, Loss: 1.5472, Val Acc: 64.42%\n",
      "Epoch 3/5, Loss: 1.4938, Val Acc: 64.42%\n",
      "Epoch 4/5, Loss: 1.4489, Val Acc: 64.42%\n",
      "Epoch 5/5, Loss: 1.4109, Val Acc: 64.42%\n",
      "Final Validation Accuracy for trigram: 64.42%\n",
      "\n",
      "==== Training quadgram model ====\n",
      "Epoch 1/5, Loss: 1.5663, Val Acc: 62.77%\n",
      "Epoch 2/5, Loss: 1.5071, Val Acc: 64.42%\n",
      "Epoch 3/5, Loss: 1.4554, Val Acc: 64.42%\n",
      "Epoch 4/5, Loss: 1.4114, Val Acc: 64.42%\n",
      "Epoch 5/5, Loss: 1.3741, Val Acc: 64.42%\n",
      "Final Validation Accuracy for quadgram: 64.42%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example constants (adjust as needed)\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "num_classes = 5\n",
    "max_len = 100  # example max length for padding\n",
    "\n",
    "# Data encoding functions\n",
    "def encode_ngrams(sentences, ngram2id, n, max_len):\n",
    "    encoded = []\n",
    "    for sent in sentences:\n",
    "        indices = []\n",
    "        for i in range(len(sent) - (n - 1)):\n",
    "            ngram = \"_\".join(sent[i:i+n])\n",
    "            idx = ngram2id.get(ngram, ngram2id['<unk>'])\n",
    "            indices.append(idx)\n",
    "        while len(indices) < max_len:\n",
    "            indices.append(ngram2id['<pad>'])\n",
    "        encoded.append(indices[:max_len])\n",
    "    return torch.tensor(encoded)\n",
    "\n",
    "# Use the factory and classifier from earlier\n",
    "# create_ngram_classifiers(...) and NgramClassifier class assumed already defined\n",
    "\n",
    "# Build datasets and loaders\n",
    "def build_loader(x_tensor, y_tensor):\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loaders = {\n",
    "    'bigram': DataLoader(train_bigram_dataset, batch_size=batch_size, shuffle=True),\n",
    "    'trigram': DataLoader(train_trigram_dataset, batch_size=batch_size, shuffle=True),\n",
    "    'quadgram': DataLoader(train_quadgram_dataset, batch_size=batch_size, shuffle=True)\n",
    "}\n",
    "\n",
    "val_loaders = {\n",
    "    'bigram': DataLoader(val_bigram_dataset, batch_size=batch_size),\n",
    "    'trigram': DataLoader(val_trigram_dataset, batch_size=batch_size),\n",
    "    'quadgram': DataLoader(val_quadgram_dataset, batch_size=batch_size)\n",
    "}\n",
    "\n",
    "# Create models\n",
    "models = create_ngram_classifiers(\n",
    "    (bigram_embedding_matrix, bigram2id),\n",
    "    (trigram_embedding_matrix, trigram2id),\n",
    "    (quadgram_embedding_matrix, quadgram2id),\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Create separate optimizers & loss\n",
    "criterions = {k: nn.CrossEntropyLoss() for k in models}\n",
    "optimizers = {\n",
    "    k: torch.optim.Adam(models[k].parameters(), lr=learning_rate)\n",
    "    for k in models\n",
    "}\n",
    "\n",
    "# Evaluation function\n",
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            logits = model(batch_x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training loop for all models\n",
    "for ngram in ['bigram', 'trigram', 'quadgram']:\n",
    "    print(f\"\\n==== Training {ngram} model ====\")\n",
    "    model = models[ngram]\n",
    "    optimizer = optimizers[ngram]\n",
    "    criterion = criterions[ngram]\n",
    "    train_loader = train_loaders[ngram]\n",
    "    val_loader = val_loaders[ngram]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        val_acc = predict(model, val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Val Acc: {val_acc * 100:.2f}%\")\n",
    "\n",
    "    final_acc = predict(model, val_loader)\n",
    "    print(f\"Final Validation Accuracy for {ngram}: {final_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 469, 0: 90, 3: 78, 2: 48, 1: 43})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“nbenNLP”",
   "language": "python",
   "name": "bennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
